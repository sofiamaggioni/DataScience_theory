{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "767bd69b-7483-4ca3-8590-ea06e1f7e38d",
   "metadata": {},
   "source": [
    "# Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31128065-48cb-4114-b2ef-74ab14f1488f",
   "metadata": {},
   "source": [
    "__Gradient descent__: technique to  solve a number of optimization problems from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06be3821-8108-41a1-ba29-34641eec45a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# The Idea Behind Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ba96ab-afed-49db-8680-780022b8cffe",
   "metadata": {},
   "source": [
    "The __gradient__ (if you remember your calculus, this is the vector\n",
    "of partial derivatives) gives the input direction in which the function most quickly\n",
    "increases.\n",
    "\n",
    "One approach to maximizing a function is to pick a random starting \n",
    "point, compute the gradient, take a small step in the direction of the gradient (i.e., th \r\n",
    "direction that causes the function to increase the most), and repeat with the n w\r\n",
    "starting point. Similarly, you can try to minimize a function by taking small steps in\r\n",
    "the opposite direc.tion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604a797f-cf0c-4c11-9f8b-e69ba8fdbb42",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Estimating the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e402bf-a71b-422e-9c69-11b46f383b1f",
   "metadata": {},
   "source": [
    "Per stimare il gradiente:\n",
    "- rapporto incrementale $ f'(x)=\\frac{f(x + h) - f(x)}{h}$\n",
    "- calcolare derivata\n",
    "- estimate derivatives by evaluating the difference quotient for a \n",
    "very smal l$\\epsilon$e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a417cc-3469-4a84-8fc1-3d83d3a8ef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rapporto incrementale \n",
    "def difference_quotient(f, x, h):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "\n",
    "#stima derivate\n",
    "def partial_difference_quotient(f, v, i, h):\n",
    "    \"\"\"compute the ith partial difference quotient of f at v\"\"\"\n",
    "    w = [v_j + (h if j == i else 0) for j, v_j in enumerate(v)]\n",
    "    return (f(w) - f(v)) / h\n",
    "\n",
    "def estimate_gradient(f, v, h=0.00001):\n",
    "    return [partial_difference_quotient(f, v, i, h) for i, _ in enumerate(v)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f2e6e5-5209-4b32-bf93-088ffcd57b30",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Using the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf565878-12cd-4e36-a3bb-6cd1c66f51ac",
   "metadata": {},
   "source": [
    "Let’s use gradients to find the minimum among all three-dimensional vectors. We’ll just pick a random starting point\n",
    "and then take tiny steps in the opposite direction of the gradient until we reach a point where the gradient is very small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50c48c7d-c744-4972-8fb4-6ce42a842be9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'magnitude' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m gradient \u001b[38;5;241m=\u001b[39m sum_of_squares_gradient(v) \u001b[38;5;66;03m# compute the gradient at v\u001b[39;00m\n\u001b[0;32m     17\u001b[0m next_v \u001b[38;5;241m=\u001b[39m step(v, gradient, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.01\u001b[39m) \u001b[38;5;66;03m# take a negative gradient step\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdistance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m tolerance: \u001b[38;5;66;03m# stop if we're converging\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     20\u001b[0m v \u001b[38;5;241m=\u001b[39m next_v \u001b[38;5;66;03m# continue if we're not\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m, in \u001b[0;36mdistance\u001b[1;34m(v, w)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdistance\u001b[39m(v, w):\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmagnitude\u001b[49m(vector_subtract(v, w))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'magnitude' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "    \n",
    "def step(v, direction, step_size):\n",
    "    \"\"\"move step_size in the direction from v\"\"\"\n",
    "    return [v_i + step_size * direction_i for v_i, direction_i in zip(v, direction)]\n",
    "    \n",
    "def sum_of_squares_gradient(v):\n",
    "    return [2 * v_i for v_i in v]\n",
    "    \n",
    "# pick a random starting point\n",
    "v = [random.randint(-10,10) for i in range(3)]\n",
    "tolerance = 0.0000001\n",
    "while True:\n",
    "    gradient = sum_of_squares_gradient(v) # compute the gradient at v\n",
    "    next_v = step(v, gradient, -0.01) # take a negative gradient step\n",
    "    if distance(next_v, v) < tolerance: # stop if we're converging\n",
    "        break\n",
    "    v = next_v # continue if we're not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd9e725-0ce9-49b4-838f-8bcd4fe8def0",
   "metadata": {},
   "source": [
    "# Choosing the Right Step Size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff33e4a-7ae2-4ada-b451-49e766e6cd06",
   "metadata": {},
   "source": [
    "Although the rationale for moving against the gradient is clear, how far to move is\n",
    "not. \n",
    "\n",
    "Best Technique: At each step, choosing the step size that minimizes the value of the objective\r\n",
    "function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a880a5-e691-46a4-9b5b-bb54138873f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe(f):\n",
    "    \"\"\"return a new function that's the same as f,\n",
    "    except that it outputs infinity whenever f produces an error\"\"\"\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            return float('inf') # this means \"infinity\" in Python\n",
    "    return safe_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3e4ef2-9ea6-49ee-ad65-0492dc320479",
   "metadata": {},
   "source": [
    "# Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618c1c90-a7bc-442c-bafe-267ec33dcc1a",
   "metadata": {},
   "source": [
    "In the general case, we have some target_fn that we want to minimize, and we also have its gradient_fn. For example, the target_fn could represent the errors in a model as a function of its parameters, and we might want to find the parameters that make the errors as small as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d6541c-6c24-4003-b6a4-8ad7e53d9caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    \"\"\"use gradient descent to find theta that minimizes target function\"\"\"\n",
    "    \n",
    "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "    \n",
    "    theta = theta_0 # set theta to initial value\n",
    "    target_fn = safe(target_fn) # safe version of target_fn\n",
    "    value = target_fn(theta) # value we're minimizing\n",
    "    \n",
    "    while True:\n",
    "        gradient = gradient_fn(theta)\n",
    "        next_thetas = [step(theta, gradient, -step_size) for step_size in step_sizes]\n",
    "        \n",
    "        # choose the one that minimizes the error function\n",
    "         next_theta = min(next_thetas, key=target_fn)\n",
    "         next_value = target_fn(next_theta)\n",
    "        \n",
    "         # stop if we're \"converging\"\n",
    "         if abs(value - next_value) < tolerance:\n",
    "             return theta\n",
    "         else:\n",
    "         theta, value = next_theta, next_valu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
